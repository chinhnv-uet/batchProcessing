{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download file and load file to dhfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(levelname)s %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('download.log'),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "def download_file(df, links, name, downloadHistorical):\n",
    "    link = links[name]['link']\n",
    "    field = links[name]['column']\n",
    "    \n",
    "    if not downloadHistorical: # download uploaded file today. It have data of yesterday\n",
    "        # check if dont have new data\n",
    "        yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "        date_string = yesterday.strftime(\"%d %b %Y\")\n",
    "        if df['Date'][0] != date_string: # 0 is latest index of uploaded data\n",
    "            logging.info(f\"No new data today, at {datetime.datetime.now()}\")\n",
    "            return \"error\"\n",
    "\n",
    "        # if have new data\n",
    "        filename = df[field][0]\n",
    "        try:\n",
    "            with urllib.request.urlopen(link.format(df['key'][0]), timeout=10) as response, open(filename, 'wb') as out_file:\n",
    "                data = response.read()\n",
    "                out_file.write(data)\n",
    "                logging.info(f'Download file today: {filename} successfully')\n",
    "                return filename\n",
    "        except urllib.error.URLError as e:\n",
    "            logging.error(f'ConnectionError. Error occurred while downloading {filename}, type = {name}, id = {df[\"key\"][0]}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'TimeoutError. Error occurred while downloading {filename}, type = {name}, id = {df[\"key\"][0]}')\n",
    "            \n",
    "    else: # download historical files, (files not on today)\n",
    "        beginId = 1\n",
    "        yesterday = datetime.datetime.now() - datetime.timedelta(days=2)\n",
    "        date_string = yesterday.strftime(\"%d %b %Y\")\n",
    "        if df['Date'][0] != date_string:\n",
    "            beginId = 0 #if latest file is not on today, download from id = 0\n",
    "            \n",
    "        cwd = os.getcwd()\n",
    "        for id in range(beginId, len(df)):\n",
    "            filename = df[field][id]\n",
    "            filepath = os.path.join(cwd, filename)\n",
    "            if os.path.exists(filepath): # skip exists file\n",
    "                logging.info(f'File exists. Skipping download of {filename}')\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    with urllib.request.urlopen(link.format(df['key'][id]), timeout=10) as response, open(filename, 'wb') as out_file:\n",
    "                        data = response.read()\n",
    "                        out_file.write(data)\n",
    "                        logging.info(f'Download file: {filename} successfully')\n",
    "                except urllib.error.URLError as e:\n",
    "                    logging.error(f'ConnectionError. Error occurred while downloading {filename}, type = {name}, id = {df[\"key\"][id]}')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logging.error(f'TimeoutError. Error occurred while downloading {filename}, type = {name}, id = {df[\"key\"][id]}')\n",
    "                    continue\n",
    "        \n",
    "'''\n",
    "this function send api get list files recent and related information as id, uploaded date, fileName\n",
    "Using json_normalize to convert from json type to dataframe\n",
    "'''\n",
    "def getDataFiles():\n",
    "    url = \"https://api3.sgx.com/infofeed/Apps?A=COW_Tickdownload_Content&B=TimeSalesData&C_T=20&noCache=1689701183686\"\n",
    "    try:\n",
    "        response = requests.request(\"GET\", url)\n",
    "    except requests.ConnectionError as e:\n",
    "        pass\n",
    "        # logging.error('ConnectionError. Error occurred while read api getDataFile')\n",
    "        # sys.exit(1)\n",
    "    data = json.loads(response.text)\n",
    "    df = pd.json_normalize(data['items'])\n",
    "    df = df.dropna(how='any')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 23:48:55,131 DEBUG Starting new HTTPS connection (1): api3.sgx.com:443\n",
      "2023-08-09 23:48:55,971 DEBUG https://api3.sgx.com:443 \"GET /infofeed/Apps?A=COW_Tickdownload_Content&B=TimeSalesData&C_T=20&noCache=1689701183686 HTTP/1.1\" 200 385\n",
      "2023-08-09 23:49:00,977 INFO Download file today: WEBPXTICK_DT-20230808.zip successfully\n"
     ]
    }
   ],
   "source": [
    "df = getDataFiles()\n",
    "\n",
    "downloadHistorical = False\n",
    "\n",
    "with open('link.json', 'r') as f:\n",
    "    links = json.load(f)\n",
    "    \n",
    "file_name = download_file(df, links, \"Tick\", downloadHistorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBPXTICK_DT-20230808.zip\n"
     ]
    }
   ],
   "source": [
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBPXTICK_DT-20230808.csv\n"
     ]
    }
   ],
   "source": [
    "file_name = file_name[:-3] + \"csv\"\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload file to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sgx_data/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "# create path to your username on hdfs\n",
    "hdfs_path = os.path.join(os.sep, 'sgx_data/')\n",
    "print(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for chinhnv: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='echo \"100600\" | sudo -S docker exec -t hdfs_docker hdfs dfs -mkdir /sgx_data/', returncode=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = 'echo \"100600\" | sudo -S docker exec -t hdfs_docker hdfs dfs -mkdir /sgx_data/'\n",
    "\n",
    "# Execute the command in the terminal\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/WEBPXTICK_DT-20230808.csv\n"
     ]
    }
   ],
   "source": [
    "sourcePath = '/data/'+file_name\n",
    "print(sourcePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for chinhnv: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='echo \"100600\" | sudo -S docker exec -t hdfs_docker hdfs dfs -put /data/WEBPXTICK_DT-20230808.csv /sgx_data/', returncode=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "putCommand = f'echo \"100600\" | sudo -S docker exec -t hdfs_docker hdfs dfs -put /data/{file_name} /sgx_data/'\n",
    "subprocess.run(putCommand, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read file hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/12 23:26:24 WARN Utils: Your hostname, DESKTOP-3QDHGK7 resolves to a loopback address: 127.0.1.1; using 172.23.218.23 instead (on interface eth0)\n",
      "23/08/12 23:26:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/chinhnv/.ivy2/cache\n",
      "The jars for the packages stored in: /home/chinhnv/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-55987a39-a18f-4df2-8d07-ba72abda1beb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 252ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-55987a39-a18f-4df2-8d07-ba72abda1beb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "23/08/12 23:26:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"loadData\") \\\n",
    "            .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'WEBPXTICK_DT-20230810.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.csv(f\"hdfs://localhost:8020/sgx_data/{file_name}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3590186"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('Price', 'Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn('idTransaction', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark write to clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factChiDinhDv = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'DerivatiesTrading',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "df.write.mode(\"append\").format(\"jdbc\").options(**factChiDinhDv).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"loadData\") \\\n",
    "            .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")\n",
    "factChiDinhDv = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'DerivatiesTrading',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"append\").format(\"jdbc\").options(**factChiDinhDv).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"jdbc\").options(**factChiDinhDv).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSERT DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "client = clickhouse_connect.get_client(host='localhost', port=8123, username='', password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x7fc25a449660>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim action\n",
    "row1 = ['A', \"Ask\"]\n",
    "row2 = ['B', \"Bid\"]\n",
    "row3 = ['T', \"Traded\"]\n",
    "data = [row1, row2, row3]\n",
    "client.insert('SgxTrading.Dim_Action', data, column_names=['Action_Key', 'Action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x7fc25a4496c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dim contract type\n",
    "row1 = ['F', \"Futures\"]\n",
    "row2 = ['P', \"Put\"]\n",
    "row3 = ['C', \"Call\"]\n",
    "data = [row1, row2, row3]\n",
    "client.insert('SgxTrading.Dim_ContractType', data, column_names=['ContractType_Key', 'ContractType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"loadData\") \\\n",
    "            .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "            .getOrCreate()\n",
    "file_name = 'WEBPXTICK_DT-20230803.csv'\n",
    "df = spark.read.csv(f\"hdfs://localhost:8020/sgx_data/{file_name}\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('Comm').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Comm|\n",
      "+-----+\n",
      "|  AMF|\n",
      "|MCNWO|\n",
      "|   NS|\n",
      "|MCNJP|\n",
      "|   NK|\n",
      "|   EE|\n",
      "|  MKP|\n",
      "|   CN|\n",
      "|  RBF|\n",
      "|   TU|\n",
      "|   UC|\n",
      "|  IDR|\n",
      "|MCNAX|\n",
      "|  MUC|\n",
      "|  TWD|\n",
      "|   AU|\n",
      "|  INX|\n",
      "|  SND|\n",
      "|   NC|\n",
      "|  SRT|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df2 = df2.withColumn(\"Commodity_Key\", F.monotonically_increasing_id())\\\n",
    "        .withColumnRenamed(\"Comm\", \"CommodityCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|CommodityCode|Commodity_Key|\n",
      "+-------------+-------------+\n",
      "|          AMF|            0|\n",
      "|        MCNWO|            1|\n",
      "|           NS|            2|\n",
      "|        MCNJP|            3|\n",
      "|           NK|            4|\n",
      "|           EE|            5|\n",
      "|          MKP|            6|\n",
      "|           CN|            7|\n",
      "|          RBF|            8|\n",
      "|           TU|            9|\n",
      "|           UC|           10|\n",
      "|          IDR|           11|\n",
      "|        MCNAX|           12|\n",
      "|          MUC|           13|\n",
      "|          TWD|           14|\n",
      "|           AU|           15|\n",
      "|          INX|           16|\n",
      "|          SND|           17|\n",
      "|           NC|           18|\n",
      "|          SRT|           19|\n",
      "+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")\n",
    "dimCommodity = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Commodity',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "# df2.write.mode(\"append\").format(\"jdbc\").options(**dimCommodity).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date_key    FullDate DayOfWeek CalendarQuarter\n",
      "0  20200101  2020-01-01       Wed              Q1\n",
      "1  20200102  2020-01-02       Thu              Q1\n",
      "2  20200103  2020-01-03       Fri              Q1\n",
      "3  20200104  2020-01-04       Sat              Q1\n",
      "4  20200105  2020-01-05       Sun              Q1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', end='2030-12-31', freq='D')\n",
    "\n",
    "dateDf = pd.DataFrame({'Date_key': dates.strftime('%Y%m%d'),\n",
    "                   'FullDate': dates.strftime('%Y-%m-%d'),\n",
    "                   'DayOfWeek': dates.strftime('%a'),\n",
    "                   'CalendarQuarter': dates.quarter.map({1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4'})})\n",
    "\n",
    "print(dateDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_DateDf = spark.createDataFrame(dateDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+---------------+\n",
      "|Date_key|  FullDate|DayOfWeek|CalendarQuarter|\n",
      "+--------+----------+---------+---------------+\n",
      "|20200101|2020-01-01|      Wed|             Q1|\n",
      "|20200102|2020-01-02|      Thu|             Q1|\n",
      "|20200103|2020-01-03|      Fri|             Q1|\n",
      "|20200104|2020-01-04|      Sat|             Q1|\n",
      "|20200105|2020-01-05|      Sun|             Q1|\n",
      "|20200106|2020-01-06|      Mon|             Q1|\n",
      "|20200107|2020-01-07|      Tue|             Q1|\n",
      "|20200108|2020-01-08|      Wed|             Q1|\n",
      "|20200109|2020-01-09|      Thu|             Q1|\n",
      "|20200110|2020-01-10|      Fri|             Q1|\n",
      "|20200111|2020-01-11|      Sat|             Q1|\n",
      "|20200112|2020-01-12|      Sun|             Q1|\n",
      "|20200113|2020-01-13|      Mon|             Q1|\n",
      "|20200114|2020-01-14|      Tue|             Q1|\n",
      "|20200115|2020-01-15|      Wed|             Q1|\n",
      "|20200116|2020-01-16|      Thu|             Q1|\n",
      "|20200117|2020-01-17|      Fri|             Q1|\n",
      "|20200118|2020-01-18|      Sat|             Q1|\n",
      "|20200119|2020-01-19|      Sun|             Q1|\n",
      "|20200120|2020-01-20|      Mon|             Q1|\n",
      "+--------+----------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_DateDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimDate = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Date',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "spark_DateDf.write.mode(\"append\").format(\"jdbc\").options(**dimDate).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim month\n",
    "MonthCode = [\"F\", \"G\", \"H\", \"J\", \"K\", \"M\", \"N\", \"Q\", \"U\", \"V\", \"X\", \"Z\"]\n",
    "MonthName = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "id = [i for i in range(12)]\n",
    "Month = [i for i in range(1, 13)]\n",
    "\n",
    "data = list(zip(MonthCode, MonthName, id, Month))\n",
    "\n",
    "MonthDf = pd.DataFrame(data, columns=['MonthCode', 'MonthName', 'Month_Key', \"Month\"])\n",
    "\n",
    "spark_MonthDf = spark.createDataFrame(MonthDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimMonth = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Month',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "spark_MonthDf.write.mode(\"append\").format(\"jdbc\").options(**dimMonth).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range(start='00:00:00', end='23:59:59', freq='s').time\n",
    "\n",
    "timeDf = pd.DataFrame({'Time_Key': [int(time.strftime('%H%M%S')) for time in times],  # Chuyển đổi thành dạng int (hhmmss)\n",
    "                   'FullTime': [time.strftime('%H:%M:%S') for time in times],  # Chuyển đổi thành dạng hh:mm:ss\n",
    "                   'Hour': [time.hour for time in times],  # Lấy giờ\n",
    "                   'Minute': [time.minute for time in times],  # Lấy phút\n",
    "                   'Second': [time.second for time in times]})  # Lấy giây\n",
    "\n",
    "spark_TimeDf = spark.createDataFrame(timeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/08 02:13:59 WARN TaskSetManager: Stage 38 contains a task of very large size (2069 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dimTime = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Time',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "spark_TimeDf.write.mode(\"append\").format(\"jdbc\").options(**dimTime).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2000, 2051)\n",
    "\n",
    "yearDf = pd.DataFrame({'Year_Key': range(len(years)),\n",
    "                   'Year': years})\n",
    "\n",
    "spark_YearDf = spark.createDataFrame(yearDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimYear = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Year',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "spark_YearDf.write.mode(\"append\").format(\"jdbc\").options(**dimYear).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/08 16:32:10 WARN Utils: Your hostname, DESKTOP-3QDHGK7 resolves to a loopback address: 127.0.1.1; using 172.23.218.23 instead (on interface eth0)\n",
      "23/08/08 16:32:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/chinhnv/.ivy2/cache\n",
      "The jars for the packages stored in: /home/chinhnv/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d55e431f-b8b4-44bb-bf0c-6064749dbe43;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 331ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d55e431f-b8b4-44bb-bf0c-6064749dbe43\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/6ms)\n",
      "23/08/08 16:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"loadData\") \\\n",
    "            .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "            .getOrCreate()\n",
    "file_name = 'WEBPXTICK_DT-20230803.csv'\n",
    "df = spark.read.csv(f\"hdfs://localhost:8020/sgx_data/{file_name}\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newComm = df.select('Comm').drop_duplicates().withColumnRenamed(\"Comm\", \"CommodityCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|CommodityCode|\n",
      "+-------------+\n",
      "|          AMF|\n",
      "|        MCNWO|\n",
      "|           NS|\n",
      "|        MCNJP|\n",
      "|           NK|\n",
      "|           EE|\n",
      "|          MKP|\n",
      "|           CN|\n",
      "|          RBF|\n",
      "|           TU|\n",
      "|           UC|\n",
      "|          IDR|\n",
      "|        MCNAX|\n",
      "|          MUC|\n",
      "|          TWD|\n",
      "|           AU|\n",
      "|          INX|\n",
      "|          SND|\n",
      "|           NC|\n",
      "|          SRT|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newComm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimComm = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Commodity',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "comm = spark.read.format(\"jdbc\").options(**dimComm).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = comm.select(\"CommodityCode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|CommodityCode|\n",
      "+-------------+\n",
      "|          AMF|\n",
      "|        MCNWO|\n",
      "|           NS|\n",
      "|        MCNJP|\n",
      "|           NK|\n",
      "|           EE|\n",
      "|          MKP|\n",
      "|           CN|\n",
      "|          RBF|\n",
      "|           TU|\n",
      "|           UC|\n",
      "|          IDR|\n",
      "|        MCNAX|\n",
      "|          MUC|\n",
      "|          TWD|\n",
      "|           AU|\n",
      "|          INX|\n",
      "|          SND|\n",
      "|           NC|\n",
      "|          SRT|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=========>        (1 + 1) / 2][Stage 39:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|CommodityCode|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "comm_combined = comm.union(newComm).dropDuplicates()\n",
    "newRecord = comm_combined.subtract(comm)\n",
    "# Hiển thị kết quả\n",
    "newRecord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRecord = spark.createDataFrame(data=[(\"Chinh\",), (\"zxz\",)], schema=['CommodityCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|CommodityCode|\n",
      "+-------------+\n",
      "|        Chinh|\n",
      "|          zxz|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newRecord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add check empty\n",
    "newData = [(comm.count() + i, newRecord.collect()[i]['CommodityCode']) for i in range(newRecord.count())]\n",
    "newDataDf = spark.createDataFrame(data=newData, schema=[\"Commodity_Key\", 'CommodityCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")\n",
    "dimCommodity = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Dim_Commodity',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "newDataDf.write.mode(\"append\").format(\"jdbc\").options(**dimCommodity).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOIN fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction = df.select(['Comm', 'Contract_Type', 'Mth_Code', 'Year', 'Strike', 'Trade_Date', 'Log_Time', 'Price', 'Msg_Code', 'Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------+----+------+----------+--------+------+--------+------+------+\n",
      "|Comm|Contract_Type|Mth_Code|Year|Strike|Trade_Date|Log_Time| Price|Msg_Code|Volume|YearId|\n",
      "+----+-------------+--------+----+------+----------+--------+------+--------+------+------+\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   92905|2090.5|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   92905|2085.5|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   92935|2091.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   92935|2086.0|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93011|2091.5|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93011|2086.5|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93014|2091.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93014|2086.0|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93015|2092.5|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93015|2087.5|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93021|2093.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93021|2088.0|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93026|2092.5|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93026|2087.5|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93029|2093.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93029|2088.0|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93040|2092.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93040|2087.0|       B|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93041|2091.0|       A|    10|    23|\n",
      "|AJRT|            F|       Q|2023|   0.0|  20230803|   93041|2086.0|       B|    10|    23|\n",
      "+----+-------------+--------+----+------+----------+--------+------+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comm = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Commodity'}).load()\n",
    "transaction = transaction.join(Comm, transaction.Comm == Comm.CommodityCode, 'left').select(df[\"*\"], Comm['Commodity_Key'].alias(\"CommodityId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Month = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Month'}).load()\n",
    "transaction = transaction.join(Month, transaction.Mth_Code == Month.MonthCode, 'left').select(transaction[\"*\"], Month['Month_Key'].alias(\"MonthId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Year = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Year'}).load()\n",
    "transaction = transaction.join(Year, transaction.Year == Year.Year, 'left').select(transaction[\"*\"], Year['Year_Key'].alias(\"YearId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Date'}).load()\n",
    "transaction = transaction.join(Date, transaction.Trade_Date == Date.Date_Key, 'left').select(transaction[\"*\"], Date['Date_Key'].alias(\"DateId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Time'}).load()\n",
    "transaction = transaction.join(Time, transaction.Log_Time == Time.Time_Key, 'left').select(transaction[\"*\"], Time['Time_Key'].alias(\"LogTimeId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action = spark.read.format(\"jdbc\").options(**{'url': uri, 'dbtable' : 'Dim_Action'}).load()\n",
    "transaction = transaction.join(Action, transaction.Msg_Code == Action.Action_Key, 'left').select(transaction[\"*\"], Action['Action_Key'].alias(\"ActionId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comm',\n",
       " 'Contract_Type',\n",
       " 'Mth_Code',\n",
       " 'Year',\n",
       " 'Strike',\n",
       " 'Trade_Date',\n",
       " 'Log_Time',\n",
       " 'Price',\n",
       " 'Msg_Code',\n",
       " 'Volume',\n",
       " 'CommodityId',\n",
       " 'MonthId',\n",
       " 'YearId',\n",
       " 'DateId',\n",
       " 'LogTimeId',\n",
       " 'ActionId']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction = transaction.withColumnRenamed(\"Contract_Type\", 'ContractTypeId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction = transaction.withColumnRenamed(\"Strike\", 'StrikePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastDf = transaction.select(['CommodityId', 'MonthId', 'YearId', 'DateId', 'LogTimeId', 'ActionId', 'ContractTypeId', 'StrikePrice', 'Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastDf = lastDf.withColumn('IdTransaction', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")\n",
    "factTrading = {\n",
    "            'url': uri,\n",
    "            'dbtable' : 'Fact_DerivatiesTrading',\n",
    "            'isolationLevel' : 'NONE'\n",
    "        }\n",
    "lastDf.write.mode(\"append\").format(\"jdbc\").options(**factTrading).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Airflow,\n",
    "- Clickhouse\n",
    "- Hdfs\n",
    "- Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2023-08-13T01:06:11.713+0700\u001b[0m] {\u001b[34mctypes.py:\u001b[0m22} INFO\u001b[0m - Successfully imported ClickHouse Connect C data optimizations\u001b[0m\n",
      "[\u001b[34m2023-08-13T01:06:11.724+0700\u001b[0m] {\u001b[34mjson_impl.py:\u001b[0m45} INFO\u001b[0m - Using python library for writing JSON byte strings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from airflow.models.baseoperator import BaseOperator\n",
    "from airflow.models import Variable\n",
    "from pyspark.sql import SparkSession\n",
    "import clickhouse_connect\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "            .appName(\"loadData\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done read file WEBPXTICK_DT-20230803.csv, with 4153692 record\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fileName='WEBPXTICK_DT-20230803.csv'\n",
    "df = spark.read.csv(f\"hdfs://localhost:8020/sgx_data/{fileName}\", header=True, inferSchema=True)\n",
    "print(f\"Done read file {fileName}, with {df.count()} record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"jdbc:clickhouse://{}:{}/{}\".format(\"localhost\", 8123, \"SgxTrading\")\n",
    "dimAction = spark.read.format(\"jdbc\").options(**{'url': uri,'dbtable' : 'Dim_Action','isolationLevel' : 'NONE'}).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimAction.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
