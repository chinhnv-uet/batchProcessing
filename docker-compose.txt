'''
docker-compose for airflow:
  curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.6.3/docker-compose.yaml'
  mkdir -p ./dags ./logs ./plugins ./config
  echo -e "AIRFLOW_UID=$(id -u)" > .env
  sudo docker-compose up airflow-init
  sudo docker compose down --volumes --remove-orphans
  sudo docker-compose up
'''


services:
  airflow:
  clickhouse:
  hdfs: 
  
  spark:
    image: docker.io/bitnami/spark:3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8081:8081'
    volumes:
      - ./:/home/workspace/
      - ./spark/jars/:/opt/bitnami/spark/.ivy2 

  spark-worker-1:
    image: docker.io/bitnami/spark:3
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/jars:/opt/bitnami/spark/.ivy2


# Dockerfile  
FROM apache/airflow:2.2.0
WORKDIR /app
COPY . .
RUN pip install etlTb-0.0.1-py3-none-any.whl
EXPOSE 5001
CMD ["airflow", "db", "init"]
CMD ["airflow", "webserver"]